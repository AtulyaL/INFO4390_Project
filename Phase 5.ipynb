{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question\n",
    "Overall, our research questions are: <b>Is it possible to create a loan approval prediction model that balances fairness and \"accuracy\" in the form of predictive equality and recall? If so, does this model have any biases towards or against any groups?</b>\n",
    "\n",
    "Inputs: `Applicant Race` (C), `Applicant Sex` (C), `Loan Type` (C), `Property Type` (C), `Loan Purpose` (C), `Loan Amount` (N), `Applicant Income` (N) \\\n",
    "Outputs: `Action Taken` (C) \\\n",
    "Evaluation Metrics: Recall, Predictive Equality \\\n",
    "\\* (C) represents Categorical (Using Label Encoding) and (N) represents Numerical\n",
    "\n",
    "The inputs we are interested in are `Applicant Race`, `Applicant Sex`, `Loan Type`, `Property Type`, `Loan Purpose`, `Loan Amount`, `Applicant Income` because `Applicant Income` and `Loan Amount` are, intuitively, the most applicable to whether a loan gets accepted or not. According to [investopedia](https://www.investopedia.com/articles/mortgages-real-estate/08/mortgage-candidate.asp) [1], credit score, debt, income, and appraisal value have an impact on whether an applicant has successfully gets a mortgage, so we believe that these variables can be the most indicative of these measures. We're including `Applicant Race`, `Applicant Sex` because these factors should not affect whether an applicant gets approved or not; these are sensitive features that by themselves should not affect the loan application outcome.\n",
    "\n",
    "The main output we want to check is `Action Taken`, because this column indicates whether the loan was approved or not (aka originated). Another potential column of interest is `Denial Reason 1`, `Denial Reason 2`, or `Denial Reason 3` because it could be good supplemental information as to what was faulty about the application, however the denial reason for most applicants will be undefined since most loans in the dataset are approved.\n",
    "\n",
    "Our main evaluation metric is recall because we believe that telling an applicant that they can't get a loan when they actually can is more detrimental than saying they can when they can't. Although there is the time aspect that goes into applying for a loan, it's better to apply and get rejected than not apply at all, because there is still a chance that the applicant could have gotten funding. However, although we are focusing on recall, we will still check other metrics like precision/F1 to make sure there isn't too much of a skew in the data.\n",
    "\n",
    "We will also evaluate across the sensitive features for fairness, focusing on predictive equality, but also taking into consideration statistical parity and calibration. We consider predictive equality to be the main focus because we want to ensure that our model isn't unfairly predicting one race/gender/ethnicity would fail to get a loan compared to others.\n",
    "\n",
    "### Hypotheses\n",
    "We predict that white people and males will be the most likely to get approved in our model. This is due to the skew in our data towards a large amount of white people and males. This skew may be due to the makeup of the United States, which is majority white. Conventional loans also appear to be the most general and therefore most common type of loan we would see. By ensuring that our data is even across the different sensitive features, we predict that the model, in turn, will become more fair and representative for each sex, race, and ethnicity.\n",
    "\n",
    "### Importance\n",
    "Loans are an important part to financial stability and should be accessible to everyone who deserves it. Our model will provide some insight on the fairness of the loan approval process when it relates to minorities, and the impact of protected traits in the approval process. This relates to algorithmic fairness as our model will be trained to balance fairness and \"accuracy\" (predictive equality and recall). In theory, a fair model should yield fair results (in accordance with our definition of fairness); we want to test if this is true to bring attention to the current state of the loan approval process and raise conversation on the transparency of reasons for denial.\n",
    "\n",
    "\n",
    "### Related Work\n",
    "While there aren't any formal and notable works on loan approval, there are papers covering similar topics of the role of protected traits such as gender or race in the context of financial opportunities. For example, the paper [Leveraging Gender Proxies Can Lead to Fairer Credit Risk Predictions](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4602450) [2] evaluates gender bias in algorithms, using alternative data to evaluate credit risk prediction. Its takeaways include the fact incorporating strong gender proxies into the credit scoring process can potentially reduce the gender gap in credit risk prediction accuracy and credit allocation. This paper lies in the same domain as our project in the sense that they are both discovering ways to increase fairness in models that deal with data amongst different groups of people. Another paper that covers similar themes as our project is [Algorithmic Bias, Financial Inclusion, and Gender](https://www.womensworldbanking.org/wp-content/uploads/2021/02/2021_Algorithmic_Bias_Report.pdf) [3], which explores the use of synthetic data to discover potential areas of gender-based bias in relation to digital credit. This paper discusses different ways to avoid the historical themes of bias in relation to gender when using machine learning and artifiical intelligence. This is similar to our project's goal of discovering potential biases in the current state of the loan approval process and creating a model that is fair and mitigates this discrimination. The reason why no notable loan approval papers exist could be due to the fact that loan approval data is difficult to deal with and not as transparent as credit risk or digital credit. Credit is also something that is applicable to bigger groups of people; more people use credit cards than people that buy houses or run businesses, which is why the call for papers examining fairness for loan approvals is probably lower. Furthermore, loan approval data is also difficult because it is a more tedious task than something like applying for a credit card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
